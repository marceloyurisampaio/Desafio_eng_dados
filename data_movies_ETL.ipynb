{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdJO3bIm5rAQ"
      },
      "source": [
        "**Nesse notebook é feito o processo de Extração, Transformação e Carregamento dos dados da Amazon e da Netflix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhw5BVuk5rAR"
      },
      "source": [
        "**Inicializando a sessão do Spark**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Java OpenJDK 11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Definir variáveis de ambiente para Java\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
        "\n",
        "# Baixar e instalar Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
        "!ls -l spark-3.5.2-bin-hadoop3\n",
        "\n",
        "# Definir variáveis de ambiente para Spark\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.5.2-bin-hadoop3'\n",
        "os.environ['PATH'] += ':/content/spark-3.5.2-bin-hadoop3/bin'\n",
        "\n",
        "!pip install pyspark\n",
        "\n",
        "#Upload do arquivo json da chave API.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!pip install kaggle\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Criando o diretório .kaggle\n",
        "os.makedirs('/content/.kaggle', exist_ok=True)\n",
        "# Movendo o arquivo kaggle.json para o diretório correto\n",
        "os.rename('kaggle.json', '/content/.kaggle/kaggle.json')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-E_d1u4Q5-Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ix4a64zw5rAR"
      },
      "outputs": [],
      "source": [
        "#Importação e inicialização da sessão do spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Window, Row\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Movies_ETL') \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5irgRE75rAS"
      },
      "source": [
        "**NETFLIX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iywjukk5rAT"
      },
      "source": [
        "**Extraindo os dados do banco de dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpvaEpiX5rAT"
      },
      "outputs": [],
      "source": [
        "#Baixar os arquivos diretamente do Kaggle\n",
        "!kaggle datasets download -d netflix-inc/netflix-prize-data\n",
        "\n",
        "# Caminho para o arquivo zip\n",
        "zip_file_path = '/content/netflix-prize-data.zip'\n",
        "\n",
        "# Descompacta o arquivo (Nesse momento pode levar um pouco mais de tempo)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('netflix_data')\n",
        "\n",
        "# Caminho para o arquivo CSV\n",
        "csv_file_path1 = '/content/netflix_data/combined_data_1.txt'\n",
        "csv_file_path2 = '/content/netflix_data/combined_data_2.txt'\n",
        "csv_file_path3 = '/content/netflix_data/combined_data_3.txt'\n",
        "csv_file_path4 = '/content/netflix_data/combined_data_4.txt'\n",
        "csv_file_path5 = '/content/netflix_data/movie_titles.csv'\n",
        "\n",
        "#Dados combinados\n",
        "rdd1 = sc.textFile(csv_file_path1)\n",
        "rdd2 = sc.textFile(csv_file_path2)\n",
        "rdd3 = sc.textFile(csv_file_path3)\n",
        "rdd4 = sc.textFile(csv_file_path4)\n",
        "\n",
        "#Dados dos títulos\n",
        "titles_schema = StructType([\n",
        "                StructField(\"movieID\", IntegerType(), True),\n",
        "                StructField(\"year_release\", IntegerType(), True),\n",
        "                StructField(\"title\", StringType(), True)])\n",
        "#Leitura do arquivo de títulos\n",
        "df_movies_titles = spark.read.csv(csv_file_path5,titles_schema, header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls1mN27h5rAU"
      },
      "source": [
        "**Transformando o dataframe da NETFLIX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P9sd244g5rAU"
      },
      "outputs": [],
      "source": [
        "#Função de transformação dos dados combinados de entrada para dataframe\n",
        "def transfom2DF_combined (rdd):\n",
        "    def parse_line(line):\n",
        "        if ':' in line:\n",
        "            parts = line.split(\":\")\n",
        "            # Cria a tupla (movieID, customer_id, rating, year)\n",
        "            return [(int(parts[0]), -1, -1, -1)]\n",
        "        else:\n",
        "            parts = line.split(\",\")\n",
        "            # Cria a tupla (movieID, customer_id, rating, year)\n",
        "            return [(None, int(parts[0]), int(parts[1]), parts[2])]\n",
        "\n",
        "    rdd_structured = rdd.flatMap(parse_line)\n",
        "\n",
        "    schema = StructType([\n",
        "             StructField(\"movieID\", IntegerType(), True),\n",
        "             StructField(\"customer_id\", IntegerType(), True),\n",
        "             StructField(\"rating\", IntegerType(), True),\n",
        "             StructField(\"review_date\", StringType(), False)])\n",
        "\n",
        "    df = spark.createDataFrame(rdd_structured, schema)\n",
        "\n",
        "    #Enumerando as linhas para auxiliar no processo de completar com os moviesID\n",
        "    df = df.withColumn(\"index\", monotonically_increasing_id())\n",
        "\n",
        "    #Criando uma janela de indices e completando com o último valor não nulo visto\n",
        "    window = Window.orderBy(\"index\").rowsBetween(-sys.maxsize, 0)\n",
        "    fill_with = last(df['movieID'], True).over(window)\n",
        "    df = df.withColumn('movieID', fill_with)\n",
        "\n",
        "    #Eliminando Coluna auxiliar\n",
        "    df = df.drop('index')\n",
        "\n",
        "    #Eliminando linhas auxiliares que contem números negativos.\n",
        "    df = df.filter(df['customer_id']>0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5m3pG2485rAV"
      },
      "outputs": [],
      "source": [
        "#Transformando os dados combinados de entrada em DataFrame\n",
        "df1_curated = transfom2DF_combined(rdd1)\n",
        "df2_curated = transfom2DF_combined(rdd2)\n",
        "df3_curated = transfom2DF_combined(rdd3)\n",
        "df4_curated = transfom2DF_combined(rdd4)\n",
        "\n",
        "#Unindo todos os DF em um unico DataFrame\n",
        "temp = df1_curated.union(df2_curated)\n",
        "temp = temp.union(df3_curated)\n",
        "dfcomb_curated = temp.union(df4_curated)\n",
        "\n",
        "##Comando a critério do usuário##\n",
        "#dfcomb_curated.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6ArDBdp-5rAX"
      },
      "outputs": [],
      "source": [
        "#Unindo os dois dataframes para ter um só com os títulos dos filmes\n",
        "df_netflix = dfcomb_curated.join(df_movies_titles, \"movieID\", how = \"left\")\n",
        "\n",
        "#Eliminando a coluna de movie_id\n",
        "df_netflix = df_netflix.drop(\"movieID\")\n",
        "\n",
        "#Adicionando uma coluna \"company\" para identificar os dados como sendo da Netflix\n",
        "df_netflix = df_netflix.withColumn(\"company\",lit(\"netflix\"))\n",
        "\n",
        "#Eliminando possíveis valores nulos\n",
        "df_netflix = df_netflix.dropna(how=\"any\")\n",
        "\n",
        "##Comando a critério do usuário##\n",
        "#df_netflix.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV7RFeCL5rAX"
      },
      "outputs": [],
      "source": [
        "##Comando a critério do usuário##\n",
        "#Extraindo o dataframe e e carregando numa camada intermediaria do datalake\n",
        "#df_netflix.write.mode('overwrite').parquet(\"/content/df_netflix_curated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhwg9M2H5rAY"
      },
      "source": [
        "**AMAZON**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDYIn8G45rAY"
      },
      "source": [
        "**Extraindo os dados do banco de dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlTuehdL5rAY"
      },
      "outputs": [],
      "source": [
        "#Baixar os arquivos diretamente do Kaggle\n",
        "!kaggle datasets download -d cynthiarempel/amazon-us-customer-reviews-dataset\n",
        "\n",
        "# Caminho para o arquivo zip\n",
        "zip_file_path = '/content/amazon-us-customer-reviews-dataset.zip'\n",
        "\n",
        "# Descompacta o arquivo(Nesse momento pode levar um pouco mais de tempo, normal mais de 20 min)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('amazon_data')\n",
        "\n",
        "# Caminho para o arquivo CSV\n",
        "csv_file_path1 = '/content/amazon_data/amazon_reviews_us_Digital_Video_Download_v1_00.tsv'\n",
        "csv_file_path2 = '/content/amazon_data/amazon_reviews_us_Video_DVD_v1_00.tsv'\n",
        "csv_file_path3 = '/content/amazon_data/amazon_reviews_us_Video_v1_00.tsv'\n",
        "\n",
        "#Extraindo os dados dos arquivos Tsv\n",
        "df1_raw = spark.read.csv(csv_file_path1, sep= '\\t', header = True)\n",
        "df2_raw = spark.read.csv(csv_file_path2, sep=  '\\t', header = True)\n",
        "df3_raw = spark.read.csv(csv_file_path3,sep= '\\t', header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8m8xKkW5rAY"
      },
      "source": [
        "**Transformando o dataframe da AMAZON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GxHSBNBj5rAY"
      },
      "outputs": [],
      "source": [
        "#Unindo so dataframes e um só\n",
        "df = df1_raw.union(df2_raw)\n",
        "df_amazon = df.union(df3_raw)\n",
        "\n",
        "##Comando a critério do usuário##\n",
        "#df_amazon.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "knFwfpQo5rAY"
      },
      "outputs": [],
      "source": [
        "#Filtrando para dados somente com lingua inglesa\n",
        "df_amazon = df_amazon.where(df_amazon.marketplace == \"US\")\n",
        "#Eliminar as colunas que não serão necessárias\n",
        "df_amazon = df_amazon.drop(\"marketplace\",\"review_id\",\"product_id\",\"product_parent\", \"product_category\",\"helpful_votes\", \"total_votes\", \"vine\", \\\n",
        "                           \"verified_purchase\", \"review_headline\", \"review_body\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v0hvBcRK5rAY"
      },
      "outputs": [],
      "source": [
        "#Eliminando possíveis valores nulos\n",
        "df_amazon = df_amazon.dropna(how=\"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N8kpzw6t5rAZ"
      },
      "outputs": [],
      "source": [
        "#Agora que as colunas estão filtradas, vamos transformar os formatos das entradas das colunas para manipula-las\n",
        "df_amazon = df_amazon.withColumnRenamed(\"star_rating\",\"rating\")\n",
        "df_amazon = df_amazon.withColumnRenamed(\"product_title\",\"title\")\n",
        "df_amazon = df_amazon.withColumn(\"customer_id\", col(\"customer_id\").cast('integer').alias(\"customer_id\"))\n",
        "df_amazon = df_amazon.withColumn(\"rating\", col(\"rating\").cast('integer').alias(\"rating\"))\n",
        "df_amazon = df_amazon.withColumn(\"review_date\",to_date(col(\"review_date\"),\"yyyy-MM-dd\").alias(\"review_date\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9w39nc7T5rAa"
      },
      "outputs": [],
      "source": [
        "#Adicionando uma coluna \"company\" para identificar os dados como sendo da Amazon\n",
        "df_amazon = df_amazon.withColumn(\"company\",lit(\"amazon\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyrydc7G5rAa"
      },
      "outputs": [],
      "source": [
        "##Comando a critério do usuário##\n",
        "#Extraindo o dataframe e salvando na camada intermediária do datalake\n",
        "#df_amazon.write.mode('overwrite').parquet(\"/content/df_amazon_curated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vck07chx5rAa"
      },
      "source": [
        "**Unindo os dois dataframes para transforma-los**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Antes de unir os dois dataframes é necessário que as colunas sejam as mesmas e na mesma ordem\n",
        "#Ordenando as colunas e preenchendo com valor sem sentido a coluna do ano de lançamento da Amazon\n",
        "df_netflix = df_netflix.select(\"customer_id\",\"title\",\"rating\",\"review_date\",\"year_release\",\"company\")\n",
        "df_amazon = df_amazon.withColumn(\"year_release\", lit(-1))\n",
        "df_amazon = df_amazon.select(\"customer_id\",\"title\",\"rating\",\"review_date\",\"year_release\",\"company\")\n",
        "\n",
        "#Unindo os dois dataframes\n",
        "df_movies = df_amazon.union(df_netflix)\n",
        "\n",
        "##Comando a critério do usuário##\n",
        "#df_movies.write.mode('overwrite').parquet(\"/content/df_movies\")\n",
        "#df_movies.show()"
      ],
      "metadata": {
        "id": "-pGx5AETg60F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Objetivo das seguintes transformações é padronizar os dados de ambas os dataframes para que os dados de entrada possam ser comparados para responder as perguntas de negócio\n",
        "\n",
        "#Particionando para melhorar a performance do processo\n",
        "df_movies = df_movies.repartition(400)\n",
        "\n",
        "#Transformando todas as letras para minusculo e criando uma coluna de título padronizado\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", lower(col(\"title\")))\n",
        "\n",
        "#A partir desse ponto utilizaremos expressões regulares da biblioteca regex para padronização dos títulos\n",
        "#Iniciando pela identificação e extração da informação de Season/Part/Volume do filme\n",
        "#Extrair a informação de Season/Part/Volume do filme\n",
        "\n",
        "#Determinando o padrão dos números que serão encontrados na string. Os dados da Amazon são correspondentes\n",
        "#a 20 anos de reviews dos filmes, então considerei que o valor máximo que um filme poderia atingir é 20\n",
        "#[1-9]([0-9])+ seleciona qualquer padrão númerico com 1 algarismo ou dois. ? indica que essa expressão numérica pode ou não ocorrer.\n",
        "#Isso seguido das possíveis strings indicando o número por extenso em inglês.\n",
        "number = ('([1-9]([0-9]+)?|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|'\n",
        "\t          'thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|'\n",
        "\t          'first|second|third|fourth|fifth|sixth|seventh|eighth|nineth|tenth|'\n",
        "\t          'eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|'\n",
        "\t          'eighteenth|nineteenth|twentieth)')\n",
        "\n",
        "#Padrão de indicação de season, part, series ou volume\n",
        "#String season ou part ou series ou vol seguida de qualquer padrão alfanumérico\n",
        "words = '(season|part|series|vol(\\w+)?)'\n",
        "\n",
        "#Situações que existem, a string seguida do número ou o número seguido da string\n",
        "#(?<= ...) Lookbehind positivo: corresponde a uma posição após um padrão específico\n",
        "after = '(?<=' + words + ' +)' + number\n",
        "#(?= ...) Lookahead positivo: corresponde a uma posição antes de um padrão específico\n",
        "before = number + '(?= +' + words + ')'\n",
        "\n",
        "#Utilizando a função de extração do regex para extrair o padrão com número após e pegando a correspondência completa (index 0)\n",
        "number_after = regexp_extract(df_movies[\"cleanTitle\"], after, 0)\n",
        "condition = number_after != ''\n",
        "number_before = regexp_extract(df_movies[\"cleanTitle\"], before, 0)\n",
        "\n",
        "#Criando uma coluna para guardar o valor da season encontrado.\n",
        "df_movies = df_movies.withColumn(\"part\", when(condition, number_after).otherwise(number_before))\n",
        "\n",
        "#Depois da separação da informação da Season, agora retirando essa informação do título padronizado\n",
        "remove_num_after = regexp_replace(df_movies[\"cleanTitle\"], words + ' ?' + after, '')\n",
        "remove_num_before = regexp_replace(df_movies[\"cleanTitle\"], before + ' ' + words , '')\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", when(condition, remove_num_after).otherwise(remove_num_before))\n",
        "\n",
        "#Removendo os textos que iniciam com parentesis ou cochetes para posteriormente adicionar de volta a informação da season.\n",
        "# Ex. movie title [2024]\n",
        "no_parenthesis= regexp_extract (df_movies[\"cleanTitle\"], r'(^[^(\\(|\\[)]+)', 0)\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", no_parenthesis)\n",
        "\n",
        "#Padronizando as entradas que indicam a Season, pois podem existir algumas por extenso e outra numéricos.\n",
        "df_movies = df_movies.withColumn('part', when((col('part') == 'one') | (col('part') =='first'), '1') \\\n",
        "\t                                      .when((col('part') == 'two') | (col('part') == 'second'), '2') \\\n",
        "\t                                      .when((col('part') == 'three') | (col('part') == 'third'), '3') \\\n",
        "\t                                      .when((col('part') == 'four') | (col('part') == 'fourth'), '4') \\\n",
        "\t                                      .when((col('part') == 'five') | (col('part') == 'fifth'), '5') \\\n",
        "\t                                      .when((col('part') == 'six') | (col('part') == 'sixth'), '6') \\\n",
        "\t                                      .when((col('part') == 'seven') | (col('part') == 'seventh'), '7') \\\n",
        "\t                                      .when((col('part') == 'eight') | (col('part') == 'eighth'), '8') \\\n",
        "\t                                      .when((col('part') == 'nine') | (col('part') == 'nineth'), '9') \\\n",
        "\t                                      .when((col('part') == 'ten') | (col('part') == 'tenth'), '10') \\\n",
        "\t                                      .when((col('part') == 'eleven') | (col('part') == 'eleventh'), '11') \\\n",
        "\t                                      .when((col('part') == 'twelve') | (col('part') == 'twelfth'), '12') \\\n",
        "\t                                      .when((col('part') == 'thirteen') | (col('part') == 'thirteenth'), '13') \\\n",
        "\t                                      .when((col('part') == 'fourteen') | (col('part') == 'fourteenth'), '14') \\\n",
        "\t                                      .when((col('part') == 'fifteen') | (col('part') == 'fifteenth'), '15') \\\n",
        "\t                                      .when((col('part') == 'sixteen') | (col('part') == 'sixteenth'), '16') \\\n",
        "\t                                      .when((col('part') == 'seventeen') | (col('part') == 'seventeenth'), '17') \\\n",
        "\t                                      .when((col('part') == 'eighteen') | (col('part') == 'eighteenth'), '18') \\\n",
        "\t                                      .when((col('part') == 'nineteen') | (col('part') == 'nineteenth'), '19') \\\n",
        "\t                                      .when((col('part') == 'twenty') | (col('part') == 'twentieth'), '20') \\\n",
        "\t                                      .otherwise(col('part')))\n",
        "\n",
        "#Adicionando novamente a informação de season/part/vol a string principal agora no formato padrão numérico\n",
        "concat = concat(col('cleanTitle'), lit(' '), col('part'))\n",
        "df_movies = df_movies.withColumn('cleanTitle', concat)\n",
        "\n",
        "#Removendo todos os caracteres que não são espaços em branco, alfanuméricos ou sublinhados, mantendo os sublinhados puros.\n",
        "no_punctuation = regexp_replace(df_movies[\"cleanTitle\"], r'([^\\s\\w_]|_)+', '')\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", no_punctuation)\n",
        "\n",
        "#Substituindo possíveis espaços duplos por espaço simples\n",
        "double_spaces = regexp_replace(df_movies[\"cleanTitle\"], r'  +', ' ')\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", double_spaces)\n",
        "\n",
        "#Removendo espaços em branco no inicio e no final do título que podem diferenciar uma string da outras\n",
        "df_movies = df_movies.withColumn(\"cleanTitle\", trim(col(\"cleanTitle\")))\n",
        "\n",
        "#Reorganizando as colunas do DataFrame agora com o titulo no formato padrao\n",
        "df_movies = df_movies.select(col(\"customer_id\"),col(\"cleanTitle\").alias(\"title\"),col(\"rating\"),col(\"review_date\"),col(\"year_release\"),col(\"company\"))\n",
        "\n",
        "\n",
        "##Comando a critério do usuário##\n",
        "df_movies.show()\n"
      ],
      "metadata": {
        "id": "5OMtUfMCCVj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregando o data frame final na camada refinada do Datalake"
      ],
      "metadata": {
        "id": "W0UNObrBnJ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Carregando o DataFrame final na camada refined do datalake já no formato escolhido\n",
        "df_movies.write.mode('overwrite').parquet(\"/content/df_movies_refined\")\n",
        "#A partir daqui os arquivos estão prontos para serem colocados no Data WareHouse"
      ],
      "metadata": {
        "id": "4fBpvtWZm7Cs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformando os 400 arquivos .parquet em um arquivo unico .zip\n",
        "!zip -r /content/df_movies.zip /content/df_movies_refined"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pcJilTvROmhe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}